{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "import gc\n",
    "import re\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, Flatten\n",
    "import cPickle as pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('en', vectors='en_glove_cc_300_1m_vectors')\n",
    "from itertools import izip_longest\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 25\n",
    "weights = '../Models/dnn_lstm2_epoch_019.hdf5'\n",
    "result_path = '../Test/submission_dnn_lstm2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['song_id', 'translated_names']\n",
    "songs = pd.read_csv('../New_Data/tr_songs.csv', usecols = headers) #419615\n",
    "songs['song_name'] = songs['translated_names'].map(str).apply(lambda x : ''.join([i for i in re.findall(r'[a-zA-Z_\\s]', x)]))\n",
    "songs['song_name'] = songs['song_name'].map(str).apply(lambda x : re.sub(r'\\s+',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../Data/test.csv', usecols=['id','song_id'])\n",
    "y = pd.DataFrame(test.song_id.unique(), columns=['song_id'], index=None)\n",
    "missing = y[~y.song_id.isin(songs.song_id)]\n",
    "missing_test = pd.DataFrame(columns = ['song_id', 'song_name'])\n",
    "missing_test['song_id'] = missing.loc[missing['song_id'].isin(test.song_id)].song_id\n",
    "missing_test['song_name'] = 'General Song'\n",
    "test_unique_songs = test.song_id.unique() #No. 224753\n",
    "test_unique_songs = pd.DataFrame(test_unique_songs, columns=['song_id'], index=None)\n",
    "test_songs = songs.loc[songs['song_id'].isin(test_unique_songs['song_id'])]\n",
    "duplicated_idx = test_songs.duplicated(subset='song_id', keep='first')\n",
    "test_songs = test_songs[~duplicated_idx]\n",
    "test_songs = test_songs.append(missing_test)\n",
    "test_songs = test_songs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del y, missing_test; \n",
    "del test_unique_songs;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_song_embeddings():\n",
    "    model = keras.models.load_model('../New_Data/LSTM_song_embeddings/songs_embeddings_100.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_songs_tensor(song_names, nlp, steps):\n",
    "    assert not isinstance(song_names, basestring)\n",
    "    nb_samples = len(song_names)\n",
    "    word_vec_dim = nlp(song_names[0].decode('utf8'))[0].vector.shape[0]\n",
    "    song_tensor = np.zeros((nb_samples, steps, word_vec_dim))\n",
    "    for i in xrange(len(song_names)):\n",
    "        tokens = nlp(song_names[i].decode('utf8'))\n",
    "        for j in xrange(len(tokens)):\n",
    "            if j<steps:\n",
    "                song_tensor[i,j,:] = tokens[j].vector\n",
    "\n",
    "    return song_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_generator(data, song_mapper):\n",
    "    num_rows = len(data)\n",
    "    X = np.zeros((len(data), seq_length, 300), dtype='float32')\n",
    "    count = 0\n",
    "    for row_num, row in data.iterrows():\n",
    "        X[count,] = song_mapper[row['song_id']]\n",
    "        count += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_song_mapper = dict()\n",
    "X_test = generate_songs_tensor(test_songs['song_name'], nlp, seq_length)\n",
    "test_song_mapper = dict(zip(test_songs['song_id'], X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_model = load_song_embeddings()\n",
    "song_embedding_model = Model(inputs=song_model.input,\n",
    "                                 outputs=song_model.get_layer('dense_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_song_ids_layer = Input(shape=(100,))\n",
    "intermediate_0 = Dense(64)(input_song_ids_layer)\n",
    "output_0 = Dense(1, activation='sigmoid')(intermediate_0)\n",
    "dnn_model = keras.models.Model(inputs = [input_song_ids_layer],\n",
    "                               outputs = [output_0])\n",
    "dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "dnn_model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = []\n",
    "Y_list = []\n",
    "num_complete_batches = int(math.floor(len(test)/batch_size))\n",
    "for i in range(0, num_complete_batches):\n",
    "    subset = test[i*batch_size : (i+1)*batch_size]\n",
    "    X_batch = song_embedding_model.predict(embedding_generator(subset, test_song_mapper), verbose=0)\n",
    "    predicted = dnn_model.predict_on_batch(X_batch)\n",
    "    Y_list.extend(subset.id)\n",
    "    predict_list.extend(predicted)\n",
    "if len(test) % batch_size != 0:\n",
    "    subset = test[(num_complete_batches*batch_size)-1 : len(test)-1]\n",
    "    X_batch = song_embedding_model.predict(embedding_generator(subset, test_song_mapper), verbose=0)\n",
    "    predicted = dnn_model.predict_on_batch(X_batch)\n",
    "    Y_list.extend(subset.id)\n",
    "    predict_list.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = ['id', 'target']\n",
    "new_test = pd.DataFrame(columns=headers)\n",
    "new_test['id'] = Y_list\n",
    "new_test['target'] = predict_list\n",
    "new_test['target'] = new_test['target'].apply(lambda x: 1 if x>0.5 else 0)\n",
    "new_test.to_csv(result_path, index=False, header=['id', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(new_test) is len(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
